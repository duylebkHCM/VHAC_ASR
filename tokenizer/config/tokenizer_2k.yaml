# Set up folders for reading from and writing to
data_folder: dataset/raw_data/
output_folder: ./save/tokenize_2K
transcript_path: transcripts.txt

# Path where data-specification files are stored
skip_prep: True
train_annotation: dataset/processed_label/combine_label/v2/train.json
valid_annotation: dataset/processed_label/combine_label/v2/valid.json


# Tokenizer parameters
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 2000  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
annotation_read: words # field to read

# Tokenizer object
tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_annotation>
   annotation_read: !ref <annotation_read>
   model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   character_coverage: !ref <character_coverage>
   annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]
   annotation_format: json
